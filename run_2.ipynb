{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/workspace/my_cyclegan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 7                             \t[default: 5]\n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "             data_shuffle: True                          \t[default: False]\n",
      "                 dataroot: /all_data/hdd/un_depth/semi/sample\n",
      "             dataset_mode: semi_cycle                    \n",
      "         disc_for_normals: False                         \n",
      "                  dropout: False                         \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 2                             \n",
      "                 img_freq: 40                            \t[default: 1]\n",
      "                init_type: normal                        \n",
      "           input_nc_depth: 1                             \n",
      "             input_nc_img: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "          l_cycle_A_begin: 10.0                          \t[default: 1.0]\n",
      "            l_cycle_A_end: 10.0                          \t[default: 1.0]\n",
      "          l_depth_A_begin: 0.0                           \t[default: 1.0]\n",
      "            l_depth_A_end: 0.0                           \t[default: 1.0]\n",
      "          l_depth_B_begin: 0.0                           \t[default: 1.0]\n",
      "            l_depth_B_end: 0.0                           \t[default: 1.0]\n",
      "         l_depth_max_iter: 10000                         \t[default: 20000]\n",
      "               l_identity: 0.0                           \t[default: 1.0]\n",
      "l_reconstruction_semantic: 1.0                           \n",
      "               load_epoch: last                          \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "              load_size_h: 480                           \n",
      "              load_size_w: 640                           \n",
      "                loss_freq: 30                            \t[default: 1]\n",
      "                     lr_D: 0.0002                        \n",
      "                     lr_G: 0.0001                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: 20000                         \t[default: inf]\n",
      "             max_distance: 8000.0                        \n",
      "                    model: semi_cycle_gan                \n",
      "                 n_blocks: 9                             \n",
      "           n_downsampling: 2                             \t[default: 3]\n",
      "                 n_epochs: 10                            \t[default: 1]\n",
      "           n_epochs_decay: 5                             \t[default: 1]\n",
      "               n_layers_D: 3                             \n",
      "                    n_pic: 3                             \n",
      "                     name: try_model_no_identity_no_depth_range_trnspose\t[default: test]\n",
      "                      ndf: 64                            \n",
      "                     netD: n_layers                      \n",
      "                ngf_depth: 32                            \t[default: 20]\n",
      "                  ngf_img: 32                            \n",
      "                     norm: instance                      \n",
      "             num_iter_dis: 1                             \n",
      "             num_iter_gen: 1                             \n",
      "              num_workers: 4                             \n",
      "            old_generator: False                         \n",
      "          output_nc_depth: 1                             \n",
      "            output_nc_img: 41                            \n",
      "                    phase: train                         \n",
      "          save_epoch_freq: 3                             \t[default: 10]\n",
      "          upsampling_type: transpose                     \t[default: upconv]\n",
      "                 use_blur: False                         \n",
      "             use_semantic: False                         \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201015_111305-1t5xl99j\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtry_model_no_identity_no_depth_range_trnspose\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res/runs/1t5xl99j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Dataset SemiCycleDataset was created\n",
      "The number of training images = 20000\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [SemiCycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network netG_A] Total number of parameters : 11.123 M\n",
      "[Network netG_B] Total number of parameters : 11.448 M\n",
      "[Network netD_A_depth] Total number of parameters : 2.763 M\n",
      "[Network netD_B_depth] Total number of parameters : 2.763 M\n",
      "-----------------------------------------------\n",
      "280 img procesed out of 20000, taken 1.0629498958587646 sec per 1 batch\n",
      "560 img procesed out of 20000, taken 1.0629785060882568 sec per 1 batch\n",
      "840 img procesed out of 20000, taken 1.1382815837860107 sec per 1 batch\n",
      "1120 img procesed out of 20000, taken 1.0714237689971924 sec per 1 batch\n",
      "1400 img procesed out of 20000, taken 1.073840618133545 sec per 1 batch\n",
      "1680 img procesed out of 20000, taken 1.143934965133667 sec per 1 batch\n",
      "1960 img procesed out of 20000, taken 1.0702743530273438 sec per 1 batch\n",
      "2240 img procesed out of 20000, taken 1.072495937347412 sec per 1 batch\n",
      "2520 img procesed out of 20000, taken 1.1428937911987305 sec per 1 batch\n",
      "2800 img procesed out of 20000, taken 1.0712323188781738 sec per 1 batch\n",
      "3080 img procesed out of 20000, taken 1.0695042610168457 sec per 1 batch\n",
      "3360 img procesed out of 20000, taken 1.1465604305267334 sec per 1 batch\n",
      "3640 img procesed out of 20000, taken 1.071239709854126 sec per 1 batch\n",
      "3920 img procesed out of 20000, taken 1.0721685886383057 sec per 1 batch\n",
      "4200 img procesed out of 20000, taken 1.1690194606781006 sec per 1 batch\n",
      "4480 img procesed out of 20000, taken 1.0710082054138184 sec per 1 batch\n",
      "4760 img procesed out of 20000, taken 1.0722885131835938 sec per 1 batch\n",
      "5040 img procesed out of 20000, taken 1.1479370594024658 sec per 1 batch\n",
      "5320 img procesed out of 20000, taken 1.0731163024902344 sec per 1 batch\n",
      "5600 img procesed out of 20000, taken 1.0699372291564941 sec per 1 batch\n",
      "5880 img procesed out of 20000, taken 1.1537270545959473 sec per 1 batch\n",
      "6160 img procesed out of 20000, taken 1.0769908428192139 sec per 1 batch\n",
      "6440 img procesed out of 20000, taken 1.081003189086914 sec per 1 batch\n",
      "6720 img procesed out of 20000, taken 1.1537690162658691 sec per 1 batch\n",
      "7000 img procesed out of 20000, taken 1.4033117294311523 sec per 1 batch\n",
      "7280 img procesed out of 20000, taken 1.0806078910827637 sec per 1 batch\n",
      "7560 img procesed out of 20000, taken 1.1689362525939941 sec per 1 batch\n",
      "7840 img procesed out of 20000, taken 1.0873634815216064 sec per 1 batch\n",
      "8120 img procesed out of 20000, taken 1.0781970024108887 sec per 1 batch\n",
      "8400 img procesed out of 20000, taken 1.1794297695159912 sec per 1 batch\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /all_data/hdd/un_depth/semi/sample\\\n",
    "--gpu_ids 2\\\n",
    "--name try_model_no_identity_no_depth_range_trnspose\\\n",
    "--gan_mode lsgan\\\n",
    "--upsampling_type transpose\\\n",
    "--n_blocks 9\\\n",
    "--n_downsampling 2\\\n",
    "--ngf_depth 32\\\n",
    "--ngf_img 32\\\n",
    "--ndf 64\\\n",
    "--netD n_layers\\\n",
    "--n_layers_D 3\\\n",
    "--l_depth_A_begin 0\\\n",
    "--l_depth_A_end 0\\\n",
    "--l_depth_B_begin 0\\\n",
    "--l_depth_B_end 0\\\n",
    "--l_depth_max_iter 10000\\\n",
    "--l_cycle_A_begin 10.0\\\n",
    "--l_cycle_A_end 10.0\\\n",
    "--l_identity 0\\\n",
    "--max_dataset_size 20000\\\n",
    "--data_shuffle\\\n",
    "--batch_size 7\\\n",
    "--lr_D 0.0002\\\n",
    "--lr_G 0.0001\\\n",
    "--num_iter_gen 1\\\n",
    "--norm instance\\\n",
    "--init_type normal\\\n",
    "--beta1 0.5\\\n",
    "--img_freq 40\\\n",
    "--loss_freq 30\\\n",
    "--save_epoch_freq 3\\\n",
    "--n_epochs 10\\\n",
    "--n_epochs_decay 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 8                             \t[default: 5]\n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "             data_shuffle: True                          \n",
      "                 dataroot: /all_data/hdd/un_depth/semi/sample\n",
      "             dataset_mode: semi_cycle                    \n",
      "         disc_for_normals: False                         \n",
      "                  dropout: True                          \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 3                             \t[default: 2]\n",
      "                 img_freq: 40                            \t[default: 1]\n",
      "                init_type: xavier                        \t[default: normal]\n",
      "           input_nc_depth: 1                             \n",
      "             input_nc_img: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "          l_depth_A_begin: 1.0                           \n",
      "            l_depth_A_end: 1.0                           \n",
      "         l_depth_max_iter: 30000                         \t[default: 20000]\n",
      "               l_gan_loss: 2.0                           \t[default: 1.0]\n",
      "               load_epoch: last                          \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "              load_size_h: 480                           \n",
      "              load_size_w: 640                           \n",
      "                loss_freq: 30                            \t[default: 1]\n",
      "                     lr_D: 0.0001                        \t[default: 0.0002]\n",
      "                     lr_G: 0.0005                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: 20000                         \t[default: inf]\n",
      "             max_distance: 8000.0                        \n",
      "                    model: A2B                           \t[default: semi_cycle_gan]\n",
      "                 n_blocks: 9                             \n",
      "           n_downsampling: 3                             \n",
      "                 n_epochs: 20                            \t[default: 1]\n",
      "           n_epochs_decay: 5                             \t[default: 1]\n",
      "               n_layers_D: 3                             \n",
      "                    n_pic: 3                             \n",
      "                     name: a2b_upconv                    \t[default: test]\n",
      "                      ndf: 64                            \n",
      "                     netD: n_layers                      \n",
      "                ngf_depth: 20                            \n",
      "                  ngf_img: 32                            \n",
      "                     norm: instance                      \n",
      "             num_iter_dis: 1                             \n",
      "             num_iter_gen: 3                             \t[default: 1]\n",
      "              num_workers: 8                             \n",
      "            old_generator: False                         \n",
      "          output_nc_depth: 1                             \n",
      "            output_nc_img: 41                            \n",
      "                    phase: train                         \n",
      "          save_epoch_freq: 2                             \t[default: 10]\n",
      "          upsampling_type: upconv                        \n",
      "             use_semantic: False                         \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201013_150441-2uohf9zh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33ma2b_upconv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res/runs/2uohf9zh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Dataset SemiCycleDataset was created\n",
      "The number of training images = 20000\n",
      "initialize network with xavier\n",
      "initialize network with xavier\n",
      "model [A2BModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network netG_A] Total number of parameters : 30.031 M\n",
      "[Network netD_A] Total number of parameters : 2.763 M\n",
      "-----------------------------------------------\n",
      "320 img procesed out of 20000, taken 0.9495193958282471 sec per 1 batch\n",
      "640 img procesed out of 20000, taken 0.9552624225616455 sec per 1 batch\n",
      "960 img procesed out of 20000, taken 1.083200216293335 sec per 1 batch\n",
      "1280 img procesed out of 20000, taken 0.9550576210021973 sec per 1 batch\n",
      "1600 img procesed out of 20000, taken 0.9561586380004883 sec per 1 batch\n",
      "1920 img procesed out of 20000, taken 1.0893092155456543 sec per 1 batch\n",
      "2240 img procesed out of 20000, taken 0.9582622051239014 sec per 1 batch\n",
      "2560 img procesed out of 20000, taken 0.9573240280151367 sec per 1 batch\n",
      "2880 img procesed out of 20000, taken 1.1574454307556152 sec per 1 batch\n",
      "3200 img procesed out of 20000, taken 0.9592268466949463 sec per 1 batch\n",
      "3520 img procesed out of 20000, taken 0.9605696201324463 sec per 1 batch\n",
      "3840 img procesed out of 20000, taken 1.0998752117156982 sec per 1 batch\n",
      "4160 img procesed out of 20000, taken 0.9606425762176514 sec per 1 batch\n",
      "4480 img procesed out of 20000, taken 0.9596858024597168 sec per 1 batch\n",
      "4800 img procesed out of 20000, taken 1.1008331775665283 sec per 1 batch\n",
      "5120 img procesed out of 20000, taken 0.9622857570648193 sec per 1 batch\n",
      "5440 img procesed out of 20000, taken 0.9662086963653564 sec per 1 batch\n",
      "5760 img procesed out of 20000, taken 1.1084375381469727 sec per 1 batch\n",
      "6080 img procesed out of 20000, taken 0.9619271755218506 sec per 1 batch\n",
      "6400 img procesed out of 20000, taken 0.9648029804229736 sec per 1 batch\n",
      "6720 img procesed out of 20000, taken 1.1004512310028076 sec per 1 batch\n",
      "7040 img procesed out of 20000, taken 0.9664933681488037 sec per 1 batch\n",
      "7360 img procesed out of 20000, taken 0.9674808979034424 sec per 1 batch\n",
      "7680 img procesed out of 20000, taken 1.104630947113037 sec per 1 batch\n",
      "8000 img procesed out of 20000, taken 1.235919713973999 sec per 1 batch\n",
      "8320 img procesed out of 20000, taken 0.9642746448516846 sec per 1 batch\n",
      "8640 img procesed out of 20000, taken 1.1045644283294678 sec per 1 batch\n",
      "8960 img procesed out of 20000, taken 0.9688031673431396 sec per 1 batch\n",
      "9280 img procesed out of 20000, taken 0.9667055606842041 sec per 1 batch\n",
      "9600 img procesed out of 20000, taken 1.1343588829040527 sec per 1 batch\n",
      "9920 img procesed out of 20000, taken 0.9689788818359375 sec per 1 batch\n",
      "10240 img procesed out of 20000, taken 0.9674718379974365 sec per 1 batch\n",
      "10560 img procesed out of 20000, taken 1.1106796264648438 sec per 1 batch\n",
      "10880 img procesed out of 20000, taken 0.9680590629577637 sec per 1 batch\n",
      "11200 img procesed out of 20000, taken 0.9656894207000732 sec per 1 batch\n",
      "11520 img procesed out of 20000, taken 1.1081228256225586 sec per 1 batch\n",
      "11840 img procesed out of 20000, taken 0.9639239311218262 sec per 1 batch\n",
      "12160 img procesed out of 20000, taken 0.9663064479827881 sec per 1 batch\n",
      "12480 img procesed out of 20000, taken 1.1111531257629395 sec per 1 batch\n",
      "12800 img procesed out of 20000, taken 0.9660241603851318 sec per 1 batch\n",
      "13120 img procesed out of 20000, taken 0.9649662971496582 sec per 1 batch\n",
      "13440 img procesed out of 20000, taken 1.1468191146850586 sec per 1 batch\n",
      "13760 img procesed out of 20000, taken 0.9672496318817139 sec per 1 batch\n",
      "14080 img procesed out of 20000, taken 0.9659497737884521 sec per 1 batch\n",
      "14400 img procesed out of 20000, taken 1.113128423690796 sec per 1 batch\n",
      "14720 img procesed out of 20000, taken 0.9635477066040039 sec per 1 batch\n",
      "15040 img procesed out of 20000, taken 0.9704010486602783 sec per 1 batch\n",
      "15360 img procesed out of 20000, taken 1.1116533279418945 sec per 1 batch\n",
      "15680 img procesed out of 20000, taken 0.9682273864746094 sec per 1 batch\n",
      "16000 img procesed out of 20000, taken 1.2448759078979492 sec per 1 batch\n",
      "16320 img procesed out of 20000, taken 1.1346371173858643 sec per 1 batch\n",
      "16640 img procesed out of 20000, taken 0.9670965671539307 sec per 1 batch\n",
      "16960 img procesed out of 20000, taken 0.9639945030212402 sec per 1 batch\n",
      "17280 img procesed out of 20000, taken 1.110344648361206 sec per 1 batch\n",
      "17600 img procesed out of 20000, taken 0.9661819934844971 sec per 1 batch\n",
      "17920 img procesed out of 20000, taken 0.9706463813781738 sec per 1 batch\n",
      "18240 img procesed out of 20000, taken 1.109736680984497 sec per 1 batch\n",
      "18560 img procesed out of 20000, taken 0.9673984050750732 sec per 1 batch\n",
      "18880 img procesed out of 20000, taken 0.9726691246032715 sec per 1 batch\n",
      "19200 img procesed out of 20000, taken 1.118424654006958 sec per 1 batch\n",
      "19520 img procesed out of 20000, taken 0.9684154987335205 sec per 1 batch\n",
      "19840 img procesed out of 20000, taken 0.967987060546875 sec per 1 batch\n",
      "End of epoch 1 / 25 \t Time Taken: 2579 sec\n",
      "learning rate = 0.0005000\n",
      "160 img procesed out of 20000, taken 1.1160390377044678 sec per 1 batch\n",
      "480 img procesed out of 20000, taken 0.9719963073730469 sec per 1 batch\n",
      "800 img procesed out of 20000, taken 0.9717416763305664 sec per 1 batch\n",
      "1120 img procesed out of 20000, taken 1.1220707893371582 sec per 1 batch\n",
      "1440 img procesed out of 20000, taken 0.9703991413116455 sec per 1 batch\n",
      "1760 img procesed out of 20000, taken 0.972576379776001 sec per 1 batch\n",
      "2080 img procesed out of 20000, taken 1.1106343269348145 sec per 1 batch\n",
      "2400 img procesed out of 20000, taken 0.9692561626434326 sec per 1 batch\n",
      "2720 img procesed out of 20000, taken 0.9658763408660889 sec per 1 batch\n",
      "3040 img procesed out of 20000, taken 1.1239488124847412 sec per 1 batch\n",
      "3360 img procesed out of 20000, taken 0.9704232215881348 sec per 1 batch\n",
      "3680 img procesed out of 20000, taken 0.9714217185974121 sec per 1 batch\n",
      "4000 img procesed out of 20000, taken 1.270639181137085 sec per 1 batch\n",
      "4320 img procesed out of 20000, taken 0.9698903560638428 sec per 1 batch\n",
      "4640 img procesed out of 20000, taken 0.9731137752532959 sec per 1 batch\n",
      "4960 img procesed out of 20000, taken 1.1221885681152344 sec per 1 batch\n",
      "5280 img procesed out of 20000, taken 0.9713754653930664 sec per 1 batch\n",
      "5600 img procesed out of 20000, taken 0.9940242767333984 sec per 1 batch\n",
      "5920 img procesed out of 20000, taken 1.1614964008331299 sec per 1 batch\n",
      "6240 img procesed out of 20000, taken 0.9710390567779541 sec per 1 batch\n",
      "6560 img procesed out of 20000, taken 0.9701199531555176 sec per 1 batch\n",
      "6880 img procesed out of 20000, taken 1.1418952941894531 sec per 1 batch\n",
      "7200 img procesed out of 20000, taken 0.9981460571289062 sec per 1 batch\n",
      "7520 img procesed out of 20000, taken 0.9781811237335205 sec per 1 batch\n",
      "7840 img procesed out of 20000, taken 1.1339342594146729 sec per 1 batch\n",
      "8160 img procesed out of 20000, taken 0.9713132381439209 sec per 1 batch\n",
      "8480 img procesed out of 20000, taken 0.9725990295410156 sec per 1 batch\n",
      "8800 img procesed out of 20000, taken 1.1180918216705322 sec per 1 batch\n",
      "9120 img procesed out of 20000, taken 0.9845309257507324 sec per 1 batch\n",
      "9440 img procesed out of 20000, taken 0.9733588695526123 sec per 1 batch\n",
      "9760 img procesed out of 20000, taken 1.1223766803741455 sec per 1 batch\n",
      "10080 img procesed out of 20000, taken 0.968451976776123 sec per 1 batch\n",
      "10400 img procesed out of 20000, taken 0.9742779731750488 sec per 1 batch\n",
      "10720 img procesed out of 20000, taken 1.1136534214019775 sec per 1 batch\n",
      "11040 img procesed out of 20000, taken 0.9711556434631348 sec per 1 batch\n",
      "11360 img procesed out of 20000, taken 0.9841217994689941 sec per 1 batch\n",
      "11680 img procesed out of 20000, taken 1.127615213394165 sec per 1 batch\n",
      "12000 img procesed out of 20000, taken 1.2866175174713135 sec per 1 batch\n",
      "12320 img procesed out of 20000, taken 0.9683291912078857 sec per 1 batch\n",
      "12640 img procesed out of 20000, taken 1.1138114929199219 sec per 1 batch\n",
      "12960 img procesed out of 20000, taken 0.9858019351959229 sec per 1 batch\n",
      "13280 img procesed out of 20000, taken 0.970740556716919 sec per 1 batch\n",
      "13600 img procesed out of 20000, taken 1.1211161613464355 sec per 1 batch\n",
      "13920 img procesed out of 20000, taken 0.9957230091094971 sec per 1 batch\n",
      "14240 img procesed out of 20000, taken 0.9816930294036865 sec per 1 batch\n",
      "14560 img procesed out of 20000, taken 1.1332240104675293 sec per 1 batch\n",
      "14880 img procesed out of 20000, taken 0.976353645324707 sec per 1 batch\n",
      "15200 img procesed out of 20000, taken 0.9729909896850586 sec per 1 batch\n",
      "15520 img procesed out of 20000, taken 1.1351583003997803 sec per 1 batch\n",
      "15840 img procesed out of 20000, taken 0.973834753036499 sec per 1 batch\n",
      "16160 img procesed out of 20000, taken 0.9719946384429932 sec per 1 batch\n",
      "16480 img procesed out of 20000, taken 1.1389214992523193 sec per 1 batch\n",
      "16800 img procesed out of 20000, taken 0.9734549522399902 sec per 1 batch\n",
      "17120 img procesed out of 20000, taken 0.9741313457489014 sec per 1 batch\n",
      "17440 img procesed out of 20000, taken 1.1475036144256592 sec per 1 batch\n",
      "17760 img procesed out of 20000, taken 0.9667119979858398 sec per 1 batch\n",
      "18080 img procesed out of 20000, taken 0.9728975296020508 sec per 1 batch\n",
      "18400 img procesed out of 20000, taken 1.1100349426269531 sec per 1 batch\n",
      "18720 img procesed out of 20000, taken 0.9753401279449463 sec per 1 batch\n",
      "19040 img procesed out of 20000, taken 0.9713485240936279 sec per 1 batch\n",
      "19360 img procesed out of 20000, taken 1.1195039749145508 sec per 1 batch\n",
      "19680 img procesed out of 20000, taken 0.9764018058776855 sec per 1 batch\n",
      "20000 img procesed out of 20000, taken 1.2653148174285889 sec per 1 batch\n",
      "saving the model at the end of epoch 2, iters 5000\n",
      "End of epoch 2 / 25 \t Time Taken: 2609 sec\n",
      "learning rate = 0.0005000\n",
      "320 img procesed out of 20000, taken 1.1324644088745117 sec per 1 batch\n",
      "640 img procesed out of 20000, taken 0.9808797836303711 sec per 1 batch\n",
      "960 img procesed out of 20000, taken 0.9728908538818359 sec per 1 batch\n",
      "1280 img procesed out of 20000, taken 1.1121420860290527 sec per 1 batch\n",
      "1600 img procesed out of 20000, taken 0.9720027446746826 sec per 1 batch\n",
      "1920 img procesed out of 20000, taken 0.9679219722747803 sec per 1 batch\n",
      "2240 img procesed out of 20000, taken 1.1424322128295898 sec per 1 batch\n",
      "2560 img procesed out of 20000, taken 1.0048978328704834 sec per 1 batch\n",
      "2880 img procesed out of 20000, taken 0.9798898696899414 sec per 1 batch\n",
      "3200 img procesed out of 20000, taken 1.139392614364624 sec per 1 batch\n",
      "3520 img procesed out of 20000, taken 1.008514642715454 sec per 1 batch\n",
      "3840 img procesed out of 20000, taken 0.9829854965209961 sec per 1 batch\n",
      "4160 img procesed out of 20000, taken 1.123819351196289 sec per 1 batch\n",
      "4480 img procesed out of 20000, taken 0.97554612159729 sec per 1 batch\n",
      "4800 img procesed out of 20000, taken 0.9925365447998047 sec per 1 batch\n",
      "5120 img procesed out of 20000, taken 1.146993637084961 sec per 1 batch\n",
      "5440 img procesed out of 20000, taken 0.9777040481567383 sec per 1 batch\n",
      "5760 img procesed out of 20000, taken 0.9692597389221191 sec per 1 batch\n",
      "6080 img procesed out of 20000, taken 1.1639800071716309 sec per 1 batch\n",
      "6400 img procesed out of 20000, taken 0.9676754474639893 sec per 1 batch\n",
      "6720 img procesed out of 20000, taken 0.9738926887512207 sec per 1 batch\n",
      "7040 img procesed out of 20000, taken 1.1074936389923096 sec per 1 batch\n",
      "7360 img procesed out of 20000, taken 0.9658772945404053 sec per 1 batch\n",
      "7680 img procesed out of 20000, taken 0.9658184051513672 sec per 1 batch\n",
      "8000 img procesed out of 20000, taken 1.4942975044250488 sec per 1 batch\n",
      "8320 img procesed out of 20000, taken 0.9648690223693848 sec per 1 batch\n",
      "8640 img procesed out of 20000, taken 0.966928243637085 sec per 1 batch\n",
      "8960 img procesed out of 20000, taken 1.1095271110534668 sec per 1 batch\n",
      "9280 img procesed out of 20000, taken 0.9693393707275391 sec per 1 batch\n",
      "9600 img procesed out of 20000, taken 0.967329740524292 sec per 1 batch\n",
      "9920 img procesed out of 20000, taken 1.1058638095855713 sec per 1 batch\n",
      "10240 img procesed out of 20000, taken 0.962113618850708 sec per 1 batch\n",
      "10560 img procesed out of 20000, taken 0.9624426364898682 sec per 1 batch\n",
      "10880 img procesed out of 20000, taken 1.146134614944458 sec per 1 batch\n",
      "11200 img procesed out of 20000, taken 0.9639439582824707 sec per 1 batch\n",
      "11520 img procesed out of 20000, taken 0.9676845073699951 sec per 1 batch\n",
      "11840 img procesed out of 20000, taken 1.109877347946167 sec per 1 batch\n",
      "12160 img procesed out of 20000, taken 0.9642372131347656 sec per 1 batch\n",
      "12480 img procesed out of 20000, taken 0.9630439281463623 sec per 1 batch\n",
      "12800 img procesed out of 20000, taken 1.1072630882263184 sec per 1 batch\n",
      "13120 img procesed out of 20000, taken 0.9657061100006104 sec per 1 batch\n",
      "13440 img procesed out of 20000, taken 0.9657700061798096 sec per 1 batch\n",
      "13760 img procesed out of 20000, taken 1.120549201965332 sec per 1 batch\n",
      "14080 img procesed out of 20000, taken 0.9638967514038086 sec per 1 batch\n",
      "14400 img procesed out of 20000, taken 0.9648523330688477 sec per 1 batch\n",
      "14720 img procesed out of 20000, taken 1.108281135559082 sec per 1 batch\n",
      "15040 img procesed out of 20000, taken 0.9645359516143799 sec per 1 batch\n",
      "15360 img procesed out of 20000, taken 0.9656977653503418 sec per 1 batch\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /all_data/hdd/un_depth/semi/sample\\\n",
    "--model A2B\\\n",
    "--gpu_ids 3\\\n",
    "--name a2b_upconv\\\n",
    "--gan_mode lsgan\\\n",
    "--upsampling_type upconv\\\n",
    "--n_blocks 9\\\n",
    "--n_downsampling 3\\\n",
    "--ngf_depth 20\\\n",
    "--ngf_img 32\\\n",
    "--ndf 64\\\n",
    "--netD n_layers\\\n",
    "--n_layers_D 3\\\n",
    "--l_depth_A_begin 1\\\n",
    "--l_depth_A_end 1\\\n",
    "--l_gan_loss 2\\\n",
    "--l_depth_max_iter 30000\\\n",
    "--max_dataset_size 20000\\\n",
    "--batch_size 8\\\n",
    "--lr_D 0.0001\\\n",
    "--lr_G 0.0005\\\n",
    "--num_iter_gen 3\\\n",
    "--norm instance\\\n",
    "--init_type xavier\\\n",
    "--beta1 0.5\\\n",
    "--img_freq 40\\\n",
    "--loss_freq 30\\\n",
    "--save_epoch_freq 2\\\n",
    "--n_epochs 20\\\n",
    "--n_epochs_decay 5\\\n",
    "# --netG_d2e unet_256\\\n",
    "# --net_d2e_weights './checkpoints/depth_edge_unet_1.5_weight_binatyloss/last_netG_B.pth'\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal(depth):\n",
    "    norm = np.zeros((2, depth.shape[0], depth.shape[1]))\n",
    "    dzdx = np.gradient(depth, 1, axis=0)\n",
    "    dzdy = np.gradient(depth, 1, axis=1)\n",
    "    norm[ 0, :, :] = -dzdx\n",
    "    norm[ 1, :, :] = -dzdy\n",
    "    n = np.linalg.norm(norm, axis = 0, ord=2, keepdims=True)\n",
    "    norm = norm/(n + 1e-15)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/results/max/check_tanh/A2B/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:30<00:00, 55.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/testB/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:28<00:00, 56.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainA/first/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:35<00:00, 52.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainA/second/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:47<00:00, 46.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/results/max/ugatit/A2B/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [06:41<00:00, 12.44it/s]  \n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainB/first/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [04:43<00:00, 17.66it/s]  \n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainB/second/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [05:54<00:00, 14.11it/s]  \n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/all_data/hdd/un_depth/semi/sample/trainB/second/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
