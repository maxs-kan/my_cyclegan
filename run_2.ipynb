{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/workspace/my_cyclegan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 10                            \t[default: 5]\n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: True                          \t[default: False]\n",
      "                crop_size: 256                           \n",
      "             data_shuffle: True                          \t[default: False]\n",
      "                 dataroot: /all_data/hdd/un_depth/semi/sample\n",
      "             dataset_mode: semi_cycle                    \n",
      "            deterministic: False                         \n",
      "         disc_for_normals: False                         \n",
      "                  dropout: False                         \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 1,2                           \t[default: 0,1]\n",
      "                 img_freq: 40                            \t[default: 1]\n",
      "                init_type: xavier                        \n",
      "           input_nc_depth: 1                             \n",
      "             input_nc_img: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "          l_cycle_A_begin: 30.0                          \t[default: 10.0]\n",
      "            l_cycle_A_end: 50.0                          \t[default: 10.0]\n",
      "          l_cycle_B_begin: 30.0                          \t[default: 10.0]\n",
      "            l_cycle_B_end: 50.0                          \t[default: 10.0]\n",
      "          l_depth_A_begin: 50.0                          \t[default: 5.0]\n",
      "            l_depth_A_end: 50.0                          \t[default: 0.0]\n",
      "          l_depth_B_begin: 50.0                          \t[default: 5.0]\n",
      "            l_depth_B_end: 50.0                          \t[default: 0.0]\n",
      "         l_depth_max_iter: 30000                         \t[default: 5000]\n",
      "               l_identity: 0.0                           \n",
      "l_reconstruction_semantic: 0.0                           \n",
      "               load_epoch: 2                             \t[default: last]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "              load_size_h: 480                           \n",
      "              load_size_w: 640                           \n",
      "                loss_freq: 30                            \t[default: 1]\n",
      "                     lr_D: 0.0001                        \t[default: 0.0002]\n",
      "                     lr_G: 0.0005                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: 20000                         \t[default: inf]\n",
      "             max_distance: 8000.0                        \n",
      "                    model: semi_cycle_gan                \n",
      "                 n_blocks: 9                             \n",
      "           n_downsampling: 2                             \n",
      "                 n_epochs: 10                            \t[default: 1]\n",
      "           n_epochs_decay: 5                             \t[default: 1]\n",
      "               n_layers_D: 3                             \n",
      "                    n_pic: 3                             \n",
      "                     name: obtain_stable_transpose       \t[default: test]\n",
      "                      ndf: 64                            \n",
      "                     netD: n_layers                      \n",
      "                ngf_depth: 32                            \n",
      "                  ngf_img: 32                            \n",
      "                     norm: instance                      \n",
      "             num_iter_dis: 1                             \n",
      "             num_iter_gen: 3                             \t[default: 1]\n",
      "              num_workers: 4                             \n",
      "            old_generator: False                         \n",
      "          output_nc_depth: 1                             \n",
      "            output_nc_img: 41                            \n",
      "                    phase: train                         \n",
      "          save_epoch_freq: 1                             \t[default: 10]\n",
      "          upsampling_type: transpose                     \t[default: upconv]\n",
      "                 use_blur: False                         \n",
      "        use_mean_matching: False                         \n",
      "         use_second_cycle: True                          \t[default: False]\n",
      "             use_semantic: False                         \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201025_170556-2hbpoxsq\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mobtain_stable_transpose\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res/runs/2hbpoxsq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Dataset SemiCycleDataset was created\n",
      "The number of training images = 20000\n",
      "initialize network with xavier\n",
      "initialize network with xavier\n",
      "initialize network with xavier\n",
      "initialize network with xavier\n",
      "model [SemiCycleGANModel] was created\n",
      "loading the model from ./checkpoints/obtain_stable_transpose/2_netG_A.pth\n",
      "loading the model from ./checkpoints/obtain_stable_transpose/2_netG_B.pth\n",
      "loading the model from ./checkpoints/obtain_stable_transpose/2_netD_A_depth.pth\n",
      "loading the model from ./checkpoints/obtain_stable_transpose/2_netD_B_depth.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network netG_A] Total number of parameters : 11.615 M\n",
      "[Network netG_B] Total number of parameters : 11.939 M\n",
      "[Network netD_A_depth] Total number of parameters : 2.763 M\n",
      "[Network netD_B_depth] Total number of parameters : 2.763 M\n",
      "-----------------------------------------------\n",
      "D_A loss : 0.078, D_B loss : 0.017\n",
      "400 img procesed out of 20000, taken 2.84 sec per 1 batch\n",
      "D_A loss : 0.050, D_B loss : 0.016\n",
      "800 img procesed out of 20000, taken 2.85 sec per 1 batch\n",
      "D_A loss : 0.028, D_B loss : 0.016\n",
      "1200 img procesed out of 20000, taken 2.84 sec per 1 batch\n",
      "D_A loss : 0.013, D_B loss : 0.013\n",
      "1600 img procesed out of 20000, taken 2.83 sec per 1 batch\n",
      "D_A loss : 0.011, D_B loss : 0.023\n",
      "2000 img procesed out of 20000, taken 2.83 sec per 1 batch\n",
      "D_A loss : 0.016, D_B loss : 0.016\n",
      "2400 img procesed out of 20000, taken 2.83 sec per 1 batch\n",
      "D_A loss : 0.031, D_B loss : 0.154\n",
      "2800 img procesed out of 20000, taken 2.83 sec per 1 batch\n",
      "D_A loss : 0.030, D_B loss : 0.036\n",
      "3200 img procesed out of 20000, taken 2.87 sec per 1 batch\n",
      "D_A loss : 0.028, D_B loss : 0.060\n",
      "3600 img procesed out of 20000, taken 2.83 sec per 1 batch\n",
      "D_A loss : 0.194, D_B loss : 0.058\n",
      "4000 img procesed out of 20000, taken 2.88 sec per 1 batch\n",
      "D_A loss : 0.033, D_B loss : 0.027\n",
      "4400 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.019, D_B loss : 0.008\n",
      "4800 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.014, D_B loss : 0.020\n",
      "5200 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.013, D_B loss : 0.009\n",
      "5600 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.008, D_B loss : 0.020\n",
      "6000 img procesed out of 20000, taken 2.85 sec per 1 batch\n",
      "D_A loss : 0.013, D_B loss : 0.005\n",
      "6400 img procesed out of 20000, taken 2.87 sec per 1 batch\n",
      "D_A loss : 0.017, D_B loss : 0.019\n",
      "6800 img procesed out of 20000, taken 2.88 sec per 1 batch\n",
      "D_A loss : 0.021, D_B loss : 0.026\n",
      "7200 img procesed out of 20000, taken 2.86 sec per 1 batch\n",
      "D_A loss : 0.009, D_B loss : 0.012\n",
      "7600 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.014, D_B loss : 0.006\n",
      "8000 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.031, D_B loss : 0.067\n",
      "8400 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.036, D_B loss : 0.011\n",
      "8800 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.011, D_B loss : 0.011\n",
      "9200 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.008, D_B loss : 0.010\n",
      "9600 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.007, D_B loss : 0.984\n",
      "10000 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.024, D_B loss : 0.009\n",
      "10400 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "D_A loss : 0.009, D_B loss : 0.007\n",
      "10800 img procesed out of 20000, taken 2.84 sec per 1 batch\n",
      "D_A loss : 0.005, D_B loss : 0.006\n",
      "11200 img procesed out of 20000, taken 2.98 sec per 1 batch\n",
      "D_A loss : 0.008, D_B loss : 0.014\n",
      "11600 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.028, D_B loss : 0.010\n",
      "12000 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.099, D_B loss : 0.015\n",
      "12400 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.010, D_B loss : 0.013\n",
      "12800 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.012, D_B loss : 0.012\n",
      "13200 img procesed out of 20000, taken 2.87 sec per 1 batch\n",
      "D_A loss : 0.032, D_B loss : 0.010\n",
      "13600 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.010, D_B loss : 0.006\n",
      "14000 img procesed out of 20000, taken 2.88 sec per 1 batch\n",
      "D_A loss : 0.008, D_B loss : 0.007\n",
      "14400 img procesed out of 20000, taken 2.96 sec per 1 batch\n",
      "D_A loss : 0.007, D_B loss : 0.005\n",
      "14800 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.008, D_B loss : 0.007\n",
      "15200 img procesed out of 20000, taken 2.97 sec per 1 batch\n",
      "D_A loss : 0.056, D_B loss : 0.011\n",
      "15600 img procesed out of 20000, taken 2.86 sec per 1 batch\n",
      "D_A loss : 0.011, D_B loss : 0.033\n",
      "16000 img procesed out of 20000, taken 2.84 sec per 1 batch\n",
      "D_A loss : 0.011, D_B loss : 0.005\n",
      "16400 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.024, D_B loss : 0.004\n",
      "16800 img procesed out of 20000, taken 2.84 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.048\n",
      "17200 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.010, D_B loss : 0.011\n",
      "17600 img procesed out of 20000, taken 2.94 sec per 1 batch\n",
      "D_A loss : 0.005, D_B loss : 0.008\n",
      "18000 img procesed out of 20000, taken 2.87 sec per 1 batch\n",
      "D_A loss : 0.014, D_B loss : 0.005\n",
      "18400 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.006\n",
      "18800 img procesed out of 20000, taken 2.85 sec per 1 batch\n",
      "D_A loss : 0.076, D_B loss : 0.015\n",
      "19200 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.032\n",
      "19600 img procesed out of 20000, taken 2.85 sec per 1 batch\n",
      "D_A loss : 0.013, D_B loss : 0.010\n",
      "20000 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "saving the model at the end of epoch 1, iters 2000\n",
      "End of epoch 1 / 15 \t Time Taken: 6005.70 sec\n",
      "learning rate = 0.0005000\n",
      "D_A loss : 0.020, D_B loss : 0.015\n",
      "400 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.015, D_B loss : 0.010\n",
      "800 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.013, D_B loss : 0.011\n",
      "1200 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.010\n",
      "1600 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.026, D_B loss : 0.012\n",
      "2000 img procesed out of 20000, taken 2.87 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.012\n",
      "2400 img procesed out of 20000, taken 2.88 sec per 1 batch\n",
      "D_A loss : 0.007, D_B loss : 0.005\n",
      "2800 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.008\n",
      "3200 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.057, D_B loss : 0.022\n",
      "3600 img procesed out of 20000, taken 2.85 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.023\n",
      "4000 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.008\n",
      "4400 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.005, D_B loss : 0.003\n",
      "4800 img procesed out of 20000, taken 2.86 sec per 1 batch\n",
      "D_A loss : 0.013, D_B loss : 0.004\n",
      "5200 img procesed out of 20000, taken 2.88 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.031\n",
      "5600 img procesed out of 20000, taken 2.95 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.004\n",
      "6000 img procesed out of 20000, taken 2.96 sec per 1 batch\n",
      "D_A loss : 0.008, D_B loss : 0.004\n",
      "6400 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.014, D_B loss : 0.003\n",
      "6800 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.011, D_B loss : 0.004\n",
      "7200 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.003\n",
      "7600 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.008\n",
      "8000 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.014, D_B loss : 0.002\n",
      "8400 img procesed out of 20000, taken 2.87 sec per 1 batch\n",
      "D_A loss : 0.009, D_B loss : 0.003\n",
      "8800 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.010\n",
      "9200 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.005, D_B loss : 0.003\n",
      "9600 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.007, D_B loss : 0.002\n",
      "10000 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.007, D_B loss : 0.009\n",
      "10400 img procesed out of 20000, taken 2.94 sec per 1 batch\n",
      "D_A loss : 0.026, D_B loss : 0.018\n",
      "10800 img procesed out of 20000, taken 2.95 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.020\n",
      "11200 img procesed out of 20000, taken 2.96 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.006\n",
      "11600 img procesed out of 20000, taken 2.85 sec per 1 batch\n",
      "D_A loss : 0.013, D_B loss : 0.012\n",
      "12000 img procesed out of 20000, taken 3.02 sec per 1 batch\n",
      "D_A loss : 0.029, D_B loss : 0.007\n",
      "12400 img procesed out of 20000, taken 2.94 sec per 1 batch\n",
      "D_A loss : 0.100, D_B loss : 0.016\n",
      "12800 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.011, D_B loss : 0.006\n",
      "13200 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.007, D_B loss : 0.006\n",
      "13600 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.005\n",
      "14000 img procesed out of 20000, taken 3.02 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.015\n",
      "14400 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.007, D_B loss : 0.003\n",
      "14800 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.005\n",
      "15200 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.007, D_B loss : 0.006\n",
      "15600 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.005, D_B loss : 0.002\n",
      "16000 img procesed out of 20000, taken 2.98 sec per 1 batch\n",
      "D_A loss : 0.005, D_B loss : 0.024\n",
      "16400 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.003\n",
      "16800 img procesed out of 20000, taken 2.94 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.431\n",
      "17200 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.010, D_B loss : 0.002\n",
      "17600 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.005, D_B loss : 0.002\n",
      "18000 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.007, D_B loss : 0.003\n",
      "18400 img procesed out of 20000, taken 2.88 sec per 1 batch\n",
      "D_A loss : 0.015, D_B loss : 0.004\n",
      "18800 img procesed out of 20000, taken 2.88 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.046\n",
      "19200 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.004\n",
      "19600 img procesed out of 20000, taken 2.96 sec per 1 batch\n",
      "D_A loss : 0.011, D_B loss : 0.018\n",
      "20000 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "saving the model at the end of epoch 2, iters 4000\n",
      "End of epoch 2 / 15 \t Time Taken: 6049.83 sec\n",
      "learning rate = 0.0005000\n",
      "D_A loss : 0.038, D_B loss : 0.005\n",
      "400 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.003\n",
      "800 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.004\n",
      "1200 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.016, D_B loss : 0.002\n",
      "1600 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.002\n",
      "2000 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.003\n",
      "2400 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.014, D_B loss : 0.010\n",
      "2800 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.004\n",
      "3200 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.050\n",
      "3600 img procesed out of 20000, taken 2.94 sec per 1 batch\n",
      "D_A loss : 0.015, D_B loss : 0.002\n",
      "4000 img procesed out of 20000, taken 2.87 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.004\n",
      "4400 img procesed out of 20000, taken 2.94 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.002\n",
      "4800 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.011\n",
      "5200 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.010, D_B loss : 0.011\n",
      "5600 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.002\n",
      "6000 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.048, D_B loss : 0.010\n",
      "6400 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.011, D_B loss : 0.005\n",
      "6800 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.005\n",
      "7200 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.021\n",
      "7600 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.014\n",
      "8000 img procesed out of 20000, taken 2.95 sec per 1 batch\n",
      "D_A loss : 0.001, D_B loss : 0.002\n",
      "8400 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.001\n",
      "8800 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.050, D_B loss : 0.003\n",
      "9200 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.007, D_B loss : 0.002\n",
      "9600 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.001\n",
      "10000 img procesed out of 20000, taken 3.15 sec per 1 batch\n",
      "D_A loss : 0.005, D_B loss : 0.008\n",
      "10400 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.009, D_B loss : 0.002\n",
      "10800 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.063, D_B loss : 0.001\n",
      "11200 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.015, D_B loss : 0.002\n",
      "11600 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.011\n",
      "12000 img procesed out of 20000, taken 2.94 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.016\n",
      "12400 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.002\n",
      "12800 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.005, D_B loss : 0.003\n",
      "13200 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.006\n",
      "13600 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.014, D_B loss : 0.005\n",
      "14000 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.008\n",
      "14400 img procesed out of 20000, taken 2.94 sec per 1 batch\n",
      "D_A loss : 0.006, D_B loss : 0.002\n",
      "14800 img procesed out of 20000, taken 2.89 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.001\n",
      "15200 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.010\n",
      "15600 img procesed out of 20000, taken 2.97 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.002\n",
      "16000 img procesed out of 20000, taken 3.04 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.003\n",
      "16400 img procesed out of 20000, taken 2.95 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.006\n",
      "16800 img procesed out of 20000, taken 2.94 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.002\n",
      "17200 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.005\n",
      "17600 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.080, D_B loss : 0.009\n",
      "18000 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.010, D_B loss : 0.004\n",
      "18400 img procesed out of 20000, taken 2.95 sec per 1 batch\n",
      "D_A loss : 0.014, D_B loss : 0.002\n",
      "18800 img procesed out of 20000, taken 3.05 sec per 1 batch\n",
      "D_A loss : 0.025, D_B loss : 0.021\n",
      "19200 img procesed out of 20000, taken 2.95 sec per 1 batch\n",
      "D_A loss : 0.010, D_B loss : 0.003\n",
      "19600 img procesed out of 20000, taken 2.95 sec per 1 batch\n",
      "D_A loss : 0.009, D_B loss : 0.001\n",
      "20000 img procesed out of 20000, taken 3.16 sec per 1 batch\n",
      "saving the model at the end of epoch 3, iters 6000\n",
      "End of epoch 3 / 15 \t Time Taken: 6070.84 sec\n",
      "learning rate = 0.0005000\n",
      "D_A loss : 0.007, D_B loss : 0.025\n",
      "400 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.001, D_B loss : 0.005\n",
      "800 img procesed out of 20000, taken 2.94 sec per 1 batch\n",
      "D_A loss : 0.008, D_B loss : 0.002\n",
      "1200 img procesed out of 20000, taken 2.95 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.001\n",
      "1600 img procesed out of 20000, taken 2.91 sec per 1 batch\n",
      "D_A loss : 0.003, D_B loss : 0.002\n",
      "2000 img procesed out of 20000, taken 2.88 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.006\n",
      "2400 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.039\n",
      "2800 img procesed out of 20000, taken 2.93 sec per 1 batch\n",
      "D_A loss : 0.001, D_B loss : 0.002\n",
      "3200 img procesed out of 20000, taken 2.95 sec per 1 batch\n",
      "D_A loss : 0.004, D_B loss : 0.007\n",
      "3600 img procesed out of 20000, taken 2.88 sec per 1 batch\n",
      "D_A loss : 0.010, D_B loss : 0.001\n",
      "4000 img procesed out of 20000, taken 2.96 sec per 1 batch\n",
      "D_A loss : 0.002, D_B loss : 0.004\n",
      "4400 img procesed out of 20000, taken 2.96 sec per 1 batch\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /all_data/hdd/un_depth/semi/sample\\\n",
    "--gpu_ids 1,2\\\n",
    "--upsampling_type transpose\\\n",
    "--name obtain_stable_transpose\\\n",
    "--continue_train\\\n",
    "--load_epoch 2\\\n",
    "--gan_mode lsgan\\\n",
    "--use_second_cycle\\\n",
    "--num_iter_gen 3\\\n",
    "--l_depth_A_begin 50\\\n",
    "--l_depth_A_end 50\\\n",
    "--l_depth_B_begin 50\\\n",
    "--l_depth_B_end 50\\\n",
    "--l_depth_max_iter 30000\\\n",
    "--l_cycle_A_begin 30.0\\\n",
    "--l_cycle_A_end 50.0\\\n",
    "--l_cycle_B_begin 30.0\\\n",
    "--l_cycle_B_end 50.0\\\n",
    "--l_identity 0.0\\\n",
    "--max_dataset_size 20000\\\n",
    "--data_shuffle\\\n",
    "--batch_size 10\\\n",
    "--lr_D 0.0001\\\n",
    "--lr_G 0.0005\\\n",
    "--img_freq 40\\\n",
    "--loss_freq 30\\\n",
    "--save_epoch_freq 1\\\n",
    "--n_epochs 10\\\n",
    "--n_epochs_decay 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 2                             \t[default: 5]\n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "             data_shuffle: True                          \t[default: False]\n",
      "                 dataroot: /all_data/hdd/un_depth/semi/sample\n",
      "             dataset_mode: semi_cycle                    \n",
      "            deterministic: False                         \n",
      "         disc_for_normals: False                         \n",
      "                  dropout: False                         \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0,1                           \t[default: 2]\n",
      "                 img_freq: 1                             \n",
      "                init_type: normal                        \n",
      "           input_nc_depth: 1                             \n",
      "             input_nc_img: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "          l_cycle_A_begin: 10.0                          \t[default: 1.0]\n",
      "            l_cycle_A_end: 10.0                          \t[default: 1.0]\n",
      "          l_cycle_B_begin: 10.0                          \t[default: 1.0]\n",
      "            l_cycle_B_end: 10.0                          \t[default: 1.0]\n",
      "          l_depth_A_begin: 5.0                           \t[default: 1.0]\n",
      "            l_depth_A_end: 0.0                           \t[default: 1.0]\n",
      "          l_depth_B_begin: 5.0                           \t[default: 1.0]\n",
      "            l_depth_B_end: 0.0                           \t[default: 1.0]\n",
      "         l_depth_max_iter: 5000                          \t[default: 20000]\n",
      "               l_identity: 0.0                           \n",
      "l_reconstruction_semantic: 0.0                           \n",
      "               load_epoch: last                          \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "              load_size_h: 480                           \n",
      "              load_size_w: 640                           \n",
      "                loss_freq: 1                             \n",
      "                     lr_D: 0.0002                        \n",
      "                     lr_G: 0.0001                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: 16                            \t[default: inf]\n",
      "             max_distance: 8000.0                        \n",
      "                    model: semi_cycle_gan                \n",
      "                 n_blocks: 9                             \n",
      "           n_downsampling: 2                             \t[default: 3]\n",
      "                 n_epochs: 1                             \n",
      "           n_epochs_decay: 1                             \n",
      "               n_layers_D: 3                             \n",
      "                    n_pic: 3                             \n",
      "                     name: test                          \n",
      "                      ndf: 64                            \n",
      "                     netD: n_layers                      \n",
      "                ngf_depth: 32                            \t[default: 20]\n",
      "                  ngf_img: 32                            \n",
      "                     norm: instance                      \n",
      "             num_iter_dis: 1                             \n",
      "             num_iter_gen: 1                             \n",
      "              num_workers: 4                             \n",
      "            old_generator: False                         \n",
      "          output_nc_depth: 1                             \n",
      "            output_nc_img: 41                            \n",
      "                    phase: train                         \n",
      "          save_epoch_freq: 1                             \t[default: 10]\n",
      "          upsampling_type: upconv                        \n",
      "                 use_blur: False                         \n",
      "        use_mean_matching: True                          \t[default: False]\n",
      "         use_second_cycle: True                          \t[default: False]\n",
      "             use_semantic: False                         \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201022_185844-lja1mj7h\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res/runs/lja1mj7h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Dataset SemiCycleDataset was created\n",
      "The number of training images = 16\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [SemiCycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network netG_A] Total number of parameters : 11.328 M\n",
      "[Network netG_B] Total number of parameters : 11.652 M\n",
      "[Network netD_A_depth] Total number of parameters : 2.763 M\n",
      "[Network netD_B_depth] Total number of parameters : 2.763 M\n",
      "-----------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 45, in <module>\n",
      "    model.optimize_param()\n",
      "  File \"/workspace/my_cyclegan/models/semi_cycle_gan.py\", line 244, in optimize_param\n",
      "    self.forward()\n",
      "  File \"/workspace/my_cyclegan/models/semi_cycle_gan.py\", line 148, in forward\n",
      "    self.rec_depth_A = self.netG_B(self.fake_depth_B)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 155, in forward\n",
      "    outputs = self.parallel_apply(replicas, inputs, kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in parallel_apply\n",
      "    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in parallel_apply\n",
      "    output.reraise()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/_utils.py\", line 395, in rera\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 5939\n",
      "ise\n",
      "    raise self.exc_type(msg)\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/workspace/my_cyclegan/models/network.py\", line 629, in forward\n",
      "    return self.dec_depth(depth)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/workspace/my_cyclegan/models/network.py\", line 518, in forward\n",
      "    return self.model(x)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 100, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/workspace/my_cyclegan/models/network.py\", line 535, in forward\n",
      "    return self.resizeconv(x)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 100, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/upsampling.py\", line 131, in forward\n",
      "    return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2991, in interpolate\n",
      "    scale_factor_list[0], scale_factor_list[1])\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.92 GiB total capacity; 1.43 GiB already allocated; 20.69 MiB free; 1.62 GiB reserved in total by PyTorch)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 1. Press ctrl-c to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Process crashed early, not syncing files\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /all_data/hdd/un_depth/semi/sample\\\n",
    "--gpu_ids 0,1\\\n",
    "--name test\\\n",
    "--use_second_cycle\\\n",
    "--use_mean_matching\\\n",
    "--gan_mode lsgan\\\n",
    "--upsampling_type upconv\\\n",
    "--n_blocks 9\\\n",
    "--n_downsampling 2\\\n",
    "--ngf_depth 32\\\n",
    "--ngf_img 32\\\n",
    "--ndf 64\\\n",
    "--netD n_layers\\\n",
    "--n_layers_D 3\\\n",
    "--l_depth_A_begin 5\\\n",
    "--l_depth_A_end 0\\\n",
    "--l_depth_B_begin 5\\\n",
    "--l_depth_B_end 0\\\n",
    "--l_depth_max_iter 5000\\\n",
    "--l_cycle_A_begin 10.0\\\n",
    "--l_cycle_A_end 10.0\\\n",
    "--l_cycle_B_begin 10.0\\\n",
    "--l_cycle_B_end 10.0\\\n",
    "--l_identity 0\\\n",
    "--max_dataset_size 16\\\n",
    "--data_shuffle\\\n",
    "--batch_size 2\\\n",
    "--lr_D 0.0002\\\n",
    "--lr_G 0.0001\\\n",
    "--num_iter_gen 1\\\n",
    "--norm instance\\\n",
    "--init_type normal\\\n",
    "--beta1 0.5\\\n",
    "--img_freq 1\\\n",
    "--loss_freq 1\\\n",
    "--save_epoch_freq 1\\\n",
    "--n_epochs 1\\\n",
    "--n_epochs_decay 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal(depth):\n",
    "    norm = np.zeros((2, depth.shape[0], depth.shape[1]))\n",
    "    dzdx = np.gradient(depth, 1, axis=0)\n",
    "    dzdy = np.gradient(depth, 1, axis=1)\n",
    "    norm[ 0, :, :] = -dzdx\n",
    "    norm[ 1, :, :] = -dzdy\n",
    "    n = np.linalg.norm(norm, axis = 0, ord=2, keepdims=True)\n",
    "    norm = norm/(n + 1e-15)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/results/max/check_tanh/A2B/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:30<00:00, 55.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/testB/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:28<00:00, 56.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainA/first/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:35<00:00, 52.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainA/second/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:47<00:00, 46.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/results/max/ugatit/A2B/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [06:41<00:00, 12.44it/s]  \n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainB/first/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [04:43<00:00, 17.66it/s]  \n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainB/second/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [05:54<00:00, 14.11it/s]  \n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/all_data/hdd/un_depth/semi/sample/trainB/second/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
