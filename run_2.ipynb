{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/workspace/my_cyclegan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python train.py --name pretrain_weights_instance\\\n",
    "# --model pretrain\\\n",
    "# --norm instance\\\n",
    "# --gpu_ids 2,3\\\n",
    "# --batch_size 16\\\n",
    "# --max_dataset_size 20000\\\n",
    "# --data_shuffle\\\n",
    "# --lr_G_A 0.0001\\\n",
    "# --lr_G_B 0.0001\\\n",
    "# --img_freq 40\\\n",
    "# --loss_freq 30\\\n",
    "# --save_epoch_freq 10\\\n",
    "# --n_epochs 10\\\n",
    "# --n_epochs_decay 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 8                             \t[default: 5]\n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "             data_shuffle: True                          \t[default: False]\n",
      "                 dataroot: /all_data/hdd/un_depth/semi/sample\n",
      "             dataset_mode: semi_cycle                    \n",
      "            deterministic: False                         \n",
      "           disc_for_depth: False                         \n",
      "         disc_for_normals: True                          \t[default: False]\n",
      "                  dropout: False                         \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 2,3                           \t[default: 0,1]\n",
      "                 img_freq: 40                            \t[default: 1]\n",
      "                init_type: xavier                        \n",
      "           input_nc_depth: 1                             \n",
      "             input_nc_img: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "          l_cycle_A_begin: 10.0                          \n",
      "            l_cycle_A_end: 10.0                          \n",
      "          l_cycle_B_begin: 10.0                          \n",
      "            l_cycle_B_end: 10.0                          \n",
      "          l_depth_A_begin: 5.0                           \n",
      "            l_depth_A_end: 0.0                           \n",
      "          l_depth_B_begin: 5.0                           \n",
      "            l_depth_B_end: 0.0                           \n",
      "         l_depth_max_iter: 10000                         \t[default: 5000]\n",
      "               l_identity: 5.0                           \t[default: 0.0]\n",
      "l_reconstruction_semantic: 0.0                           \n",
      "               load_epoch: last                          \n",
      "       load_epoch_weights: last                          \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "              load_size_h: 480                           \n",
      "              load_size_w: 640                           \n",
      "                loss_freq: 30                            \t[default: 1]\n",
      "                     lr_D: 0.0002                        \n",
      "                     lr_G: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: 20000                         \t[default: inf]\n",
      "             max_distance: 8000.0                        \n",
      "                   mean_A: 1680.1208394737955            \n",
      "                   mean_B: 2781.0011373752295            \n",
      "                    model: semi_cycle_gan                \n",
      "                 n_blocks: 9                             \n",
      "           n_downsampling: 2                             \n",
      "                 n_epochs: 10                            \t[default: 1]\n",
      "           n_epochs_decay: 5                             \t[default: 1]\n",
      "               n_layers_D: 3                             \n",
      "                    n_pic: 3                             \n",
      "                     name: mean_normals_weights_norm_synimg_depthrange\t[default: test]\n",
      "                      ndf: 64                            \n",
      "                     netD: n_layers                      \n",
      "                ngf_depth: 32                            \n",
      "                  ngf_img: 32                            \n",
      "                     norm: batch                         \t[default: instance]\n",
      "             num_iter_dis: 1                             \n",
      "             num_iter_gen: 1                             \n",
      "              num_workers: 4                             \n",
      "          output_nc_depth: 1                             \n",
      "            output_nc_img: 41                            \n",
      "                    phase: train                         \n",
      "          save_epoch_freq: 10                            \n",
      "                    std_A: 487.0543836544621             \n",
      "                    std_B: 780.4723869231325             \n",
      "          upsampling_type: transpose                     \n",
      "                 use_blur: False                         \n",
      "        use_mean_matching: False                         \n",
      "      use_petrain_weights: True                          \t[default: False]\n",
      "         use_second_cycle: True                          \t[default: False]\n",
      "             use_semantic: False                         \n",
      "           use_semi_cycle: False                         \n",
      "                  verbose: False                         \n",
      "              weights_dir: ./checkpoints/pretrain_weights_batch_norms/\t[default: ./checkpoints/pretrain_weights/]\n",
      "----------------- End -------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201112_214949-3hj5q7kz\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmean_normals_weights_norm_synimg_depthrange\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res/runs/3hj5q7kz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Dataset SemiCycleDataset was created\n",
      "The number of training images = 20000\n",
      "initialize network with xavier\n",
      "initialize network with xavier\n",
      "initialize network with xavier\n",
      "initialize network with xavier\n",
      "model [SemiCycleGANModel] was created\n",
      "loading the model from ./checkpoints/pretrain_weights_batch_norms/last.pt\n",
      "loading the model from ./checkpoints/pretrain_weights_batch_norms/last.pt\n",
      "---------- Networks initialized -------------\n",
      "[Network netG_A] Total number of parameters : 11.620 M\n",
      "[Network netG_B] Total number of parameters : 11.620 M\n",
      "[Network netD_A_normal] Total number of parameters : 2.766 M\n",
      "[Network netD_B_normal] Total number of parameters : 2.766 M\n",
      "-----------------------------------------------\n",
      "320 img procesed out of 20000, taken 1.14 sec per 1 batch\n"
     ]
    }
   ],
   "source": [
    "!python train.py --name mean_normals_weights_norm_synimg_depthrange\\\n",
    "--gpu_ids 2,3\\\n",
    "--batch_size 8\\\n",
    "--disc_for_normals\\\n",
    "--norm batch\\\n",
    "--use_petrain_weights\\\n",
    "--weights_dir ./checkpoints/pretrain_weights_batch_norms/\\\n",
    "--load_epoch_weights last\\\n",
    "--use_second_cycle\\\n",
    "--l_depth_A_begin 5\\\n",
    "--l_depth_A_end 0\\\n",
    "--l_depth_B_begin 5\\\n",
    "--l_depth_B_end 0\\\n",
    "--l_cycle_A_begin 10.0\\\n",
    "--l_cycle_A_end 10.0\\\n",
    "--l_cycle_B_begin 10.0\\\n",
    "--l_cycle_B_end 10.0\\\n",
    "--l_depth_max_iter 10000\\\n",
    "--l_identity 5.0\\\n",
    "--max_dataset_size 20000\\\n",
    "--data_shuffle\\\n",
    "--lr_D 0.0002\\\n",
    "--lr_G 0.0002\\\n",
    "--img_freq 40\\\n",
    "--loss_freq 30\\\n",
    "--save_epoch_freq 10\\\n",
    "--n_epochs 10\\\n",
    "--n_epochs_decay 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 14                            \t[default: 5]\n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "             data_shuffle: True                          \t[default: False]\n",
      "                 dataroot: /all_data/hdd/un_depth/semi/sample\n",
      "             dataset_mode: semi_cycle                    \n",
      "            deterministic: False                         \n",
      "         disc_for_normals: False                         \n",
      "                  dropout: False                         \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 1,2                           \t[default: 0,1]\n",
      "                 img_freq: 40                            \t[default: 1]\n",
      "                init_type: xavier                        \n",
      "           input_nc_depth: 1                             \n",
      "             input_nc_img: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "          l_cycle_A_begin: 30.0                          \t[default: 10.0]\n",
      "            l_cycle_A_end: 50.0                          \t[default: 10.0]\n",
      "          l_cycle_B_begin: 10.0                          \n",
      "            l_cycle_B_end: 10.0                          \n",
      "          l_depth_A_begin: 50.0                          \t[default: 5.0]\n",
      "            l_depth_A_end: 50.0                          \t[default: 0.0]\n",
      "          l_depth_B_begin: 50.0                          \t[default: 5.0]\n",
      "            l_depth_B_end: 50.0                          \t[default: 0.0]\n",
      "         l_depth_max_iter: 10000                         \t[default: 5000]\n",
      "               l_identity: 10.0                          \t[default: 0.0]\n",
      "l_reconstruction_semantic: 0.0                           \n",
      "               load_epoch: last                          \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "              load_size_h: 480                           \n",
      "              load_size_w: 640                           \n",
      "                loss_freq: 30                            \t[default: 1]\n",
      "                     lr_D: 0.0001                        \t[default: 0.0002]\n",
      "                     lr_G: 0.0005                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: 20000                         \t[default: inf]\n",
      "             max_distance: 8000.0                        \n",
      "                    model: semi_cycle_gan                \n",
      "                 n_blocks: 9                             \n",
      "           n_downsampling: 3                             \t[default: 2]\n",
      "                 n_epochs: 10                            \t[default: 1]\n",
      "           n_epochs_decay: 5                             \t[default: 1]\n",
      "               n_layers_D: 3                             \n",
      "                    n_pic: 3                             \n",
      "                     name: lsgan_3_iter_nodrop_nonormdisc\t[default: test]\n",
      "                      ndf: 64                            \n",
      "                     netD: n_layers                      \n",
      "                     netG: resnet_6blocks                \n",
      "                ngf_depth: 20                            \t[default: 32]\n",
      "                  ngf_img: 32                            \n",
      "                     norm: instance                      \n",
      "             num_iter_dis: 1                             \n",
      "             num_iter_gen: 3                             \t[default: 1]\n",
      "              num_workers: 4                             \n",
      "            old_generator: False                         \n",
      "          output_nc_depth: 1                             \n",
      "            output_nc_img: 41                            \n",
      "                    phase: train                         \n",
      "          save_epoch_freq: 1                             \t[default: 10]\n",
      "          upsampling_type: transpose                     \t[default: upconv]\n",
      "                 use_blur: False                         \n",
      "        use_mean_matching: False                         \n",
      "         use_second_cycle: False                         \n",
      "             use_semantic: False                         \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201029_190715-s7thcbj3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlsgan_3_iter_nodrop_nonormdisc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res/runs/s7thcbj3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Dataset SemiCycleDataset was created\n",
      "The number of training images = 20000\n",
      "initialize network with xavier\n",
      "initialize network with xavier\n",
      "initialize network with xavier\n",
      "initialize network with xavier\n",
      "model [SemiCycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network netG_A] Total number of parameters : 29.463 M\n",
      "[Network netG_B] Total number of parameters : 17.943 M\n",
      "[Network netD_A_depth] Total number of parameters : 2.763 M\n",
      "[Network netD_B_depth] Total number of parameters : 2.763 M\n",
      "-----------------------------------------------\n",
      "D_A loss : 0.664, D_B loss : 0.292\n",
      "560 img procesed out of 20000, taken 3.12 sec per 1 batch\n",
      "D_A loss : 0.412, D_B loss : 0.235\n",
      "1120 img procesed out of 20000, taken 3.05 sec per 1 batch\n",
      "D_A loss : 0.172, D_B loss : 0.181\n",
      "1680 img procesed out of 20000, taken 3.03 sec per 1 batch\n",
      "D_A loss : 0.171, D_B loss : 0.226\n",
      "2240 img procesed out of 20000, taken 3.03 sec per 1 batch\n",
      "D_A loss : 0.220, D_B loss : 0.140\n",
      "2800 img procesed out of 20000, taken 3.06 sec per 1 batch\n",
      "D_A loss : 0.240, D_B loss : 0.522\n",
      "3360 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.120, D_B loss : 0.221\n",
      "3920 img procesed out of 20000, taken 3.03 sec per 1 batch\n",
      "D_A loss : 0.151, D_B loss : 0.233\n",
      "4480 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.095, D_B loss : 0.167\n",
      "5040 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.120, D_B loss : 0.184\n",
      "5600 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.133, D_B loss : 0.172\n",
      "6160 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.232, D_B loss : 0.200\n",
      "6720 img procesed out of 20000, taken 3.05 sec per 1 batch\n",
      "D_A loss : 0.042, D_B loss : 0.174\n",
      "7280 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "D_A loss : 0.091, D_B loss : 0.121\n",
      "7840 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.119, D_B loss : 0.114\n",
      "8400 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.058, D_B loss : 0.181\n",
      "8960 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.088, D_B loss : 0.187\n",
      "9520 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.117, D_B loss : 0.167\n",
      "10080 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.102, D_B loss : 0.120\n",
      "10640 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.079, D_B loss : 0.210\n",
      "11200 img procesed out of 20000, taken 3.05 sec per 1 batch\n",
      "D_A loss : 0.074, D_B loss : 0.131\n",
      "11760 img procesed out of 20000, taken 3.06 sec per 1 batch\n",
      "D_A loss : 0.084, D_B loss : 0.074\n",
      "12320 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.051, D_B loss : 0.172\n",
      "12880 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.114, D_B loss : 0.136\n",
      "13440 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.089, D_B loss : 0.173\n",
      "14000 img procesed out of 20000, taken 3.34 sec per 1 batch\n",
      "D_A loss : 0.030, D_B loss : 0.125\n",
      "14560 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.057, D_B loss : 0.151\n",
      "15120 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.101, D_B loss : 0.089\n",
      "15680 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.062, D_B loss : 0.126\n",
      "16240 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.085, D_B loss : 0.079\n",
      "16800 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.045, D_B loss : 0.245\n",
      "17360 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.039, D_B loss : 0.065\n",
      "17920 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.064, D_B loss : 0.077\n",
      "18480 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.073, D_B loss : 0.159\n",
      "19040 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.058, D_B loss : 0.136\n",
      "19600 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "saving the model at the end of epoch 1, iters 1429\n",
      "End of epoch 1 / 15 \t Time Taken: 4578.14 sec\n",
      "learning rate = 0.0005000\n",
      "D_A loss : 0.606, D_B loss : 0.124\n",
      "154 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.045, D_B loss : 0.153\n",
      "714 img procesed out of 20000, taken 3.05 sec per 1 batch\n",
      "D_A loss : 0.043, D_B loss : 0.082\n",
      "1274 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.041, D_B loss : 0.081\n",
      "1834 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.053, D_B loss : 0.160\n",
      "2394 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.068, D_B loss : 0.130\n",
      "2954 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.060, D_B loss : 0.128\n",
      "3514 img procesed out of 20000, taken 3.05 sec per 1 batch\n",
      "D_A loss : 0.043, D_B loss : 0.072\n",
      "4074 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.036, D_B loss : 0.085\n",
      "4634 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.033, D_B loss : 0.063\n",
      "5194 img procesed out of 20000, taken 3.12 sec per 1 batch\n",
      "D_A loss : 0.064, D_B loss : 0.130\n",
      "5754 img procesed out of 20000, taken 3.12 sec per 1 batch\n",
      "D_A loss : 0.040, D_B loss : 0.068\n",
      "6314 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.030, D_B loss : 0.077\n",
      "6874 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.038, D_B loss : 0.067\n",
      "7434 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.069, D_B loss : 0.137\n",
      "7994 img procesed out of 20000, taken 3.38 sec per 1 batch\n",
      "D_A loss : 0.057, D_B loss : 0.119\n",
      "8554 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.062, D_B loss : 0.411\n",
      "9114 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.030, D_B loss : 0.097\n",
      "9674 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "D_A loss : 0.060, D_B loss : 0.130\n",
      "10234 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.032, D_B loss : 0.103\n",
      "10794 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.024, D_B loss : 0.093\n",
      "11354 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.081, D_B loss : 0.083\n",
      "11914 img procesed out of 20000, taken 3.06 sec per 1 batch\n",
      "D_A loss : 0.029, D_B loss : 0.080\n",
      "12474 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.018, D_B loss : 0.086\n",
      "13034 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.028, D_B loss : 0.081\n",
      "13594 img procesed out of 20000, taken 2.92 sec per 1 batch\n",
      "D_A loss : 0.041, D_B loss : 0.056\n",
      "14154 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.042, D_B loss : 0.116\n",
      "14714 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.060, D_B loss : 0.094\n",
      "15274 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.047, D_B loss : 0.105\n",
      "15834 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.064, D_B loss : 0.057\n",
      "16394 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.041, D_B loss : 0.048\n",
      "16954 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.030, D_B loss : 0.051\n",
      "17514 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.025, D_B loss : 0.059\n",
      "18074 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.027, D_B loss : 0.102\n",
      "18634 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "D_A loss : 0.024, D_B loss : 0.058\n",
      "19194 img procesed out of 20000, taken 3.05 sec per 1 batch\n",
      "D_A loss : 0.021, D_B loss : 0.147\n",
      "19754 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "saving the model at the end of epoch 2, iters 2858\n",
      "End of epoch 2 / 15 \t Time Taken: 4598.92 sec\n",
      "learning rate = 0.0005000\n",
      "D_A loss : 0.214, D_B loss : 0.049\n",
      "308 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.119, D_B loss : 0.054\n",
      "868 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.079, D_B loss : 0.048\n",
      "1428 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.070, D_B loss : 0.071\n",
      "1988 img procesed out of 20000, taken 3.34 sec per 1 batch\n",
      "D_A loss : 0.056, D_B loss : 0.070\n",
      "2548 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.117, D_B loss : 0.139\n",
      "3108 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.068, D_B loss : 0.053\n",
      "3668 img procesed out of 20000, taken 3.02 sec per 1 batch\n",
      "D_A loss : 0.113, D_B loss : 0.071\n",
      "4228 img procesed out of 20000, taken 3.12 sec per 1 batch\n",
      "D_A loss : 0.092, D_B loss : 0.095\n",
      "4788 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.071, D_B loss : 0.106\n",
      "5348 img procesed out of 20000, taken 3.12 sec per 1 batch\n",
      "D_A loss : 0.085, D_B loss : 0.064\n",
      "5908 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.069, D_B loss : 0.053\n",
      "6468 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "D_A loss : 0.116, D_B loss : 0.085\n",
      "7028 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.034, D_B loss : 0.026\n",
      "7588 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.043, D_B loss : 0.058\n",
      "8148 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "D_A loss : 0.020, D_B loss : 0.038\n",
      "8708 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.077, D_B loss : 0.090\n",
      "9268 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "D_A loss : 0.051, D_B loss : 0.077\n",
      "9828 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.292, D_B loss : 0.114\n",
      "10388 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.156, D_B loss : 0.036\n",
      "10948 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.057, D_B loss : 0.075\n",
      "11508 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.080, D_B loss : 0.062\n",
      "12068 img procesed out of 20000, taken 3.12 sec per 1 batch\n",
      "D_A loss : 0.081, D_B loss : 0.106\n",
      "12628 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "D_A loss : 0.077, D_B loss : 0.039\n",
      "13188 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.162, D_B loss : 0.078\n",
      "13748 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.058, D_B loss : 0.092\n",
      "14308 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.065, D_B loss : 0.063\n",
      "14868 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.070, D_B loss : 0.030\n",
      "15428 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.061, D_B loss : 0.064\n",
      "15988 img procesed out of 20000, taken 3.42 sec per 1 batch\n",
      "D_A loss : 0.037, D_B loss : 0.070\n",
      "16548 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.134, D_B loss : 0.034\n",
      "17108 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "D_A loss : 0.074, D_B loss : 0.035\n",
      "17668 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "D_A loss : 0.026, D_B loss : 0.041\n",
      "18228 img procesed out of 20000, taken 2.98 sec per 1 batch\n",
      "D_A loss : 0.108, D_B loss : 0.063\n",
      "18788 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.095, D_B loss : 0.033\n",
      "19348 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.030, D_B loss : 0.037\n",
      "19908 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "saving the model at the end of epoch 3, iters 4287\n",
      "End of epoch 3 / 15 \t Time Taken: 4600.08 sec\n",
      "learning rate = 0.0005000\n",
      "D_A loss : 0.024, D_B loss : 0.022\n",
      "462 img procesed out of 20000, taken 3.14 sec per 1 batch\n",
      "D_A loss : 0.209, D_B loss : 0.064\n",
      "1022 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.036, D_B loss : 0.046\n",
      "1582 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.014, D_B loss : 0.045\n",
      "2142 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.074, D_B loss : 0.047\n",
      "2702 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.038, D_B loss : 0.134\n",
      "3262 img procesed out of 20000, taken 2.90 sec per 1 batch\n",
      "D_A loss : 0.041, D_B loss : 0.040\n",
      "3822 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.026, D_B loss : 0.070\n",
      "4382 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.054, D_B loss : 0.064\n",
      "4942 img procesed out of 20000, taken 3.06 sec per 1 batch\n",
      "D_A loss : 0.084, D_B loss : 0.038\n",
      "5502 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.087, D_B loss : 0.057\n",
      "6062 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.086, D_B loss : 0.036\n",
      "6622 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.085, D_B loss : 0.064\n",
      "7182 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.052, D_B loss : 0.027\n",
      "7742 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.036, D_B loss : 0.221\n",
      "8302 img procesed out of 20000, taken 3.05 sec per 1 batch\n",
      "D_A loss : 0.016, D_B loss : 0.039\n",
      "8862 img procesed out of 20000, taken 3.07 sec per 1 batch\n",
      "D_A loss : 0.034, D_B loss : 0.072\n",
      "9422 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.099, D_B loss : 0.067\n",
      "9982 img procesed out of 20000, taken 3.41 sec per 1 batch\n",
      "D_A loss : 0.051, D_B loss : 0.041\n",
      "10542 img procesed out of 20000, taken 3.10 sec per 1 batch\n",
      "D_A loss : 0.027, D_B loss : 0.077\n",
      "11102 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.022, D_B loss : 0.053\n",
      "11662 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.062, D_B loss : 0.032\n",
      "12222 img procesed out of 20000, taken 3.08 sec per 1 batch\n",
      "D_A loss : 0.045, D_B loss : 0.024\n",
      "12782 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.029, D_B loss : 0.020\n",
      "13342 img procesed out of 20000, taken 3.09 sec per 1 batch\n",
      "D_A loss : 0.031, D_B loss : 0.094\n",
      "13902 img procesed out of 20000, taken 3.11 sec per 1 batch\n",
      "D_A loss : 0.027, D_B loss : 0.032\n",
      "14462 img procesed out of 20000, taken 3.10 sec per 1 batch\n"
     ]
    }
   ],
   "source": [
    "!python train.py --name lsgan_3_iter_nodrop_nonormdisc\\\n",
    "--gpu_ids 1,2\\\n",
    "--batch_size 14\\\n",
    "--upsampling_type transpose\\\n",
    "--num_iter_gen 3\\\n",
    "--n_downsampling 3\\\n",
    "--ngf_depth 20\\\n",
    "--l_depth_A_begin 50\\\n",
    "--l_depth_A_end 50\\\n",
    "--l_depth_B_begin 50\\\n",
    "--l_depth_B_end 50\\\n",
    "--l_depth_max_iter 10000\\\n",
    "--l_cycle_A_begin 30.0\\\n",
    "--l_cycle_A_end 50.0\\\n",
    "--l_identity 10.0\\\n",
    "--max_dataset_size 20000\\\n",
    "--data_shuffle\\\n",
    "--lr_D 0.0001\\\n",
    "--lr_G 0.0005\\\n",
    "--img_freq 40\\\n",
    "--loss_freq 30\\\n",
    "--save_epoch_freq 1\\\n",
    "--n_epochs 10\\\n",
    "--n_epochs_decay 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 2                             \t[default: 5]\n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "             data_shuffle: True                          \t[default: False]\n",
      "                 dataroot: /all_data/hdd/un_depth/semi/sample\n",
      "             dataset_mode: semi_cycle                    \n",
      "            deterministic: False                         \n",
      "         disc_for_normals: False                         \n",
      "                  dropout: False                         \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0,1                           \t[default: 2]\n",
      "                 img_freq: 1                             \n",
      "                init_type: normal                        \n",
      "           input_nc_depth: 1                             \n",
      "             input_nc_img: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "          l_cycle_A_begin: 10.0                          \t[default: 1.0]\n",
      "            l_cycle_A_end: 10.0                          \t[default: 1.0]\n",
      "          l_cycle_B_begin: 10.0                          \t[default: 1.0]\n",
      "            l_cycle_B_end: 10.0                          \t[default: 1.0]\n",
      "          l_depth_A_begin: 5.0                           \t[default: 1.0]\n",
      "            l_depth_A_end: 0.0                           \t[default: 1.0]\n",
      "          l_depth_B_begin: 5.0                           \t[default: 1.0]\n",
      "            l_depth_B_end: 0.0                           \t[default: 1.0]\n",
      "         l_depth_max_iter: 5000                          \t[default: 20000]\n",
      "               l_identity: 0.0                           \n",
      "l_reconstruction_semantic: 0.0                           \n",
      "               load_epoch: last                          \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "              load_size_h: 480                           \n",
      "              load_size_w: 640                           \n",
      "                loss_freq: 1                             \n",
      "                     lr_D: 0.0002                        \n",
      "                     lr_G: 0.0001                        \t[default: 0.0002]\n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: 16                            \t[default: inf]\n",
      "             max_distance: 8000.0                        \n",
      "                    model: semi_cycle_gan                \n",
      "                 n_blocks: 9                             \n",
      "           n_downsampling: 2                             \t[default: 3]\n",
      "                 n_epochs: 1                             \n",
      "           n_epochs_decay: 1                             \n",
      "               n_layers_D: 3                             \n",
      "                    n_pic: 3                             \n",
      "                     name: test                          \n",
      "                      ndf: 64                            \n",
      "                     netD: n_layers                      \n",
      "                ngf_depth: 32                            \t[default: 20]\n",
      "                  ngf_img: 32                            \n",
      "                     norm: instance                      \n",
      "             num_iter_dis: 1                             \n",
      "             num_iter_gen: 1                             \n",
      "              num_workers: 4                             \n",
      "            old_generator: False                         \n",
      "          output_nc_depth: 1                             \n",
      "            output_nc_img: 41                            \n",
      "                    phase: train                         \n",
      "          save_epoch_freq: 1                             \t[default: 10]\n",
      "          upsampling_type: upconv                        \n",
      "                 use_blur: False                         \n",
      "        use_mean_matching: True                          \t[default: False]\n",
      "         use_second_cycle: True                          \t[default: False]\n",
      "             use_semantic: False                         \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201022_185844-lja1mj7h\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/maxs/depth_super_res/runs/lja1mj7h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Dataset SemiCycleDataset was created\n",
      "The number of training images = 16\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [SemiCycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network netG_A] Total number of parameters : 11.328 M\n",
      "[Network netG_B] Total number of parameters : 11.652 M\n",
      "[Network netD_A_depth] Total number of parameters : 2.763 M\n",
      "[Network netD_B_depth] Total number of parameters : 2.763 M\n",
      "-----------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 45, in <module>\n",
      "    model.optimize_param()\n",
      "  File \"/workspace/my_cyclegan/models/semi_cycle_gan.py\", line 244, in optimize_param\n",
      "    self.forward()\n",
      "  File \"/workspace/my_cyclegan/models/semi_cycle_gan.py\", line 148, in forward\n",
      "    self.rec_depth_A = self.netG_B(self.fake_depth_B)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 155, in forward\n",
      "    outputs = self.parallel_apply(replicas, inputs, kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in parallel_apply\n",
      "    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in parallel_apply\n",
      "    output.reraise()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/_utils.py\", line 395, in rera\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 5939\n",
      "ise\n",
      "    raise self.exc_type(msg)\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/workspace/my_cyclegan/models/network.py\", line 629, in forward\n",
      "    return self.dec_depth(depth)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/workspace/my_cyclegan/models/network.py\", line 518, in forward\n",
      "    return self.model(x)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 100, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/workspace/my_cyclegan/models/network.py\", line 535, in forward\n",
      "    return self.resizeconv(x)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 100, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/upsampling.py\", line 131, in forward\n",
      "    return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2991, in interpolate\n",
      "    scale_factor_list[0], scale_factor_list[1])\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 10.92 GiB total capacity; 1.43 GiB already allocated; 20.69 MiB free; 1.62 GiB reserved in total by PyTorch)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 1. Press ctrl-c to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Process crashed early, not syncing files\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /all_data/hdd/un_depth/semi/sample\\\n",
    "--gpu_ids 0,1\\\n",
    "--name test\\\n",
    "--use_second_cycle\\\n",
    "--use_mean_matching\\\n",
    "--gan_mode lsgan\\\n",
    "--upsampling_type upconv\\\n",
    "--n_blocks 9\\\n",
    "--n_downsampling 2\\\n",
    "--ngf_depth 32\\\n",
    "--ngf_img 32\\\n",
    "--ndf 64\\\n",
    "--netD n_layers\\\n",
    "--n_layers_D 3\\\n",
    "--l_depth_A_begin 5\\\n",
    "--l_depth_A_end 0\\\n",
    "--l_depth_B_begin 5\\\n",
    "--l_depth_B_end 0\\\n",
    "--l_depth_max_iter 5000\\\n",
    "--l_cycle_A_begin 10.0\\\n",
    "--l_cycle_A_end 10.0\\\n",
    "--l_cycle_B_begin 10.0\\\n",
    "--l_cycle_B_end 10.0\\\n",
    "--l_identity 0\\\n",
    "--max_dataset_size 16\\\n",
    "--data_shuffle\\\n",
    "--batch_size 2\\\n",
    "--lr_D 0.0002\\\n",
    "--lr_G 0.0001\\\n",
    "--num_iter_gen 1\\\n",
    "--norm instance\\\n",
    "--init_type normal\\\n",
    "--beta1 0.5\\\n",
    "--img_freq 1\\\n",
    "--loss_freq 1\\\n",
    "--save_epoch_freq 1\\\n",
    "--n_epochs 1\\\n",
    "--n_epochs_decay 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal(depth):\n",
    "    norm = np.zeros((2, depth.shape[0], depth.shape[1]))\n",
    "    dzdx = np.gradient(depth, 1, axis=0)\n",
    "    dzdy = np.gradient(depth, 1, axis=1)\n",
    "    norm[ 0, :, :] = -dzdx\n",
    "    norm[ 1, :, :] = -dzdy\n",
    "    n = np.linalg.norm(norm, axis = 0, ord=2, keepdims=True)\n",
    "    norm = norm/(n + 1e-15)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/results/max/check_tanh/A2B/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:30<00:00, 55.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/testB/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:28<00:00, 56.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainA/first/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:35<00:00, 52.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainA/second/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:47<00:00, 46.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/results/max/ugatit/A2B/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [06:41<00:00, 12.44it/s]  \n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainB/first/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [04:43<00:00, 17.66it/s]  \n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/all_data/hdd/un_depth/semi/sample/trainB/second/'\n",
    "Depths = glob(os.path.join(path, 'depth/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [05:54<00:00, 14.11it/s]  \n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(Depths):\n",
    "    norm = get_normal(imageio.imread(d))\n",
    "    name = os.path.basename(d).split('.')[0]\n",
    "    np.save(os.path.join(path,'normal', name+'.npy'), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/all_data/hdd/un_depth/semi/sample/trainB/second/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
